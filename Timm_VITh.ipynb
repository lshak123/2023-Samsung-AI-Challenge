{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a86bc0",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625896c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import timm\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8385ec2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convit_base\n",
      "convit_small\n",
      "convit_tiny\n",
      "crossvit_9_240\n",
      "crossvit_9_dagger_240\n",
      "crossvit_15_240\n",
      "crossvit_15_dagger_240\n",
      "crossvit_15_dagger_408\n",
      "crossvit_18_240\n",
      "crossvit_18_dagger_240\n",
      "crossvit_18_dagger_408\n",
      "crossvit_base_240\n",
      "crossvit_small_240\n",
      "crossvit_tiny_240\n",
      "levit_128\n",
      "levit_128s\n",
      "levit_192\n",
      "levit_256\n",
      "levit_384\n",
      "vit_base_patch8_224\n",
      "vit_base_patch8_224_in21k\n",
      "vit_base_patch16_224\n",
      "vit_base_patch16_224_in21k\n",
      "vit_base_patch16_224_miil\n",
      "vit_base_patch16_224_miil_in21k\n",
      "vit_base_patch16_384\n",
      "vit_base_patch16_sam_224\n",
      "vit_base_patch32_224\n",
      "vit_base_patch32_224_in21k\n",
      "vit_base_patch32_384\n",
      "vit_base_patch32_sam_224\n",
      "vit_base_r50_s16_224_in21k\n",
      "vit_base_r50_s16_384\n",
      "vit_huge_patch14_224_in21k\n",
      "vit_large_patch16_224\n",
      "vit_large_patch16_224_in21k\n",
      "vit_large_patch16_384\n",
      "vit_large_patch32_224_in21k\n",
      "vit_large_patch32_384\n",
      "vit_large_r50_s32_224\n",
      "vit_large_r50_s32_224_in21k\n",
      "vit_large_r50_s32_384\n",
      "vit_small_patch16_224\n",
      "vit_small_patch16_224_in21k\n",
      "vit_small_patch16_384\n",
      "vit_small_patch32_224\n",
      "vit_small_patch32_224_in21k\n",
      "vit_small_patch32_384\n",
      "vit_small_r26_s32_224\n",
      "vit_small_r26_s32_224_in21k\n",
      "vit_small_r26_s32_384\n",
      "vit_tiny_patch16_224\n",
      "vit_tiny_patch16_224_in21k\n",
      "vit_tiny_patch16_384\n",
      "vit_tiny_r_s16_p8_224\n",
      "vit_tiny_r_s16_p8_224_in21k\n",
      "vit_tiny_r_s16_p8_384\n"
     ]
    }
   ],
   "source": [
    "#model_names = timm.list_models(pretrained=True)\n",
    "model_names = timm.list_models('*swin*',pretrained=True)\n",
    "\n",
    "for i in model_names:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b2ca74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e072d0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06049d0c",
   "metadata": {},
   "source": [
    "## Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f604b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS': 30, #Your Epochs,\n",
    "    'LR':0.0001, #Your Learning Rate,\n",
    "    'BATCH_SIZE': 16, #Your Batch Size,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4dc03d",
   "metadata": {},
   "source": [
    "## Fixed Random-Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37779d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f9b1e",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5afeb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['img_path']\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # mos column 존재 여부에 따라 값을 설정\n",
    "        mos = float(self.dataframe.iloc[idx]['mos']) if 'mos' in self.dataframe.columns else 0.0\n",
    "        \n",
    "        return img, mos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd639aba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9258fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "        # Image feature extraction using ResNet50\n",
    "        self.cnn_backbone = timm.create_model('vit_base_patch16_224_in21k',pretrained=True).to(device)\n",
    "        # Remove the last fully connected layer to get features\n",
    "        modules = list(self.cnn_backbone.children())[:-1]\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "\n",
    "        # Image quality assessment head\n",
    "        self.layer1 = nn.Linear(327680,163840)\n",
    "        self.layer2 = nn.Linear(163840,81920)\n",
    "        self.layer3 = nn.Linear(81920,40910)\n",
    "        self.layer4 = nn.Linear(40910,20455)\n",
    "        self.layer5 = nn.Linear(20455,2048)\n",
    "        self.regression_head = nn.Linear(2048, 1)  # ResNet50 last layer has 2048 features\n",
    "\n",
    "    def forward(self, images):\n",
    "        # CNN\n",
    "        features = self.cnn(images)\n",
    "        #features_flat = features.view(features.size(0), -1)\n",
    "        features_flat = features.reshape(features.size(0), -1)\n",
    "        \n",
    "        # Image quality regression\n",
    "        features_flat = self.layer1(features_flat)\n",
    "        features_flat = self.layer2(features_flat)\n",
    "        features_flat = self.layer3(features_flat)\n",
    "        features_flat = self.layer4(features_flat)\n",
    "        features_flat = self.layer5(features_flat)\n",
    "        mos = self.regression_head(features_flat)\n",
    "        return mos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4ea9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d810b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_data = pd.read_csv('../open/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f3f138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>img_path</th>\n",
       "      <th>mos</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41wy7upxzl</td>\n",
       "      <td>../open/train/41wy7upxzl.jpg</td>\n",
       "      <td>5.569231</td>\n",
       "      <td>the pink and blue really compliment each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ygujjq6xxt</td>\n",
       "      <td>../open/train/ygujjq6xxt.jpg</td>\n",
       "      <td>6.103175</td>\n",
       "      <td>love rhubarb! great colors!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wk321130q0</td>\n",
       "      <td>../open/train/wk321130q0.jpg</td>\n",
       "      <td>5.541985</td>\n",
       "      <td>i enjoy the textures and grungy feel to this. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w50dp2zjpg</td>\n",
       "      <td>../open/train/w50dp2zjpg.jpg</td>\n",
       "      <td>6.234848</td>\n",
       "      <td>i like all the different colours in this pic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l7rqfxeuh0</td>\n",
       "      <td>../open/train/l7rqfxeuh0.jpg</td>\n",
       "      <td>5.190476</td>\n",
       "      <td>i love these critters, just wish he was a litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74563</th>\n",
       "      <td>zbevd0lyox</td>\n",
       "      <td>../open/train/zbevd0lyox.jpg</td>\n",
       "      <td>5.926108</td>\n",
       "      <td>perfect balance here, in this soft serene image.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74564</th>\n",
       "      <td>w26yu6ee60</td>\n",
       "      <td>../open/train/w26yu6ee60.jpg</td>\n",
       "      <td>5.966346</td>\n",
       "      <td>very nice indeed. the sharpness and contrast a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74565</th>\n",
       "      <td>a1pts9zzdx</td>\n",
       "      <td>../open/train/a1pts9zzdx.jpg</td>\n",
       "      <td>5.718447</td>\n",
       "      <td>nice tones and color for balance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74566</th>\n",
       "      <td>pzbubeo03l</td>\n",
       "      <td>../open/train/pzbubeo03l.jpg</td>\n",
       "      <td>6.007843</td>\n",
       "      <td>i like the bold colors. nice sharp image.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74567</th>\n",
       "      <td>8c0klk5ule</td>\n",
       "      <td>../open/train/8c0klk5ule.jpg</td>\n",
       "      <td>5.599206</td>\n",
       "      <td>i am an aries and just flat out liked this ide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74568 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         img_name                      img_path       mos  \\\n",
       "0      41wy7upxzl  ../open/train/41wy7upxzl.jpg  5.569231   \n",
       "1      ygujjq6xxt  ../open/train/ygujjq6xxt.jpg  6.103175   \n",
       "2      wk321130q0  ../open/train/wk321130q0.jpg  5.541985   \n",
       "3      w50dp2zjpg  ../open/train/w50dp2zjpg.jpg  6.234848   \n",
       "4      l7rqfxeuh0  ../open/train/l7rqfxeuh0.jpg  5.190476   \n",
       "...           ...                           ...       ...   \n",
       "74563  zbevd0lyox  ../open/train/zbevd0lyox.jpg  5.926108   \n",
       "74564  w26yu6ee60  ../open/train/w26yu6ee60.jpg  5.966346   \n",
       "74565  a1pts9zzdx  ../open/train/a1pts9zzdx.jpg  5.718447   \n",
       "74566  pzbubeo03l  ../open/train/pzbubeo03l.jpg  6.007843   \n",
       "74567  8c0klk5ule  ../open/train/8c0klk5ule.jpg  5.599206   \n",
       "\n",
       "                                                comments  \n",
       "0      the pink and blue really compliment each other...  \n",
       "1                            love rhubarb! great colors!  \n",
       "2      i enjoy the textures and grungy feel to this. ...  \n",
       "3      i like all the different colours in this pic, ...  \n",
       "4      i love these critters, just wish he was a litt...  \n",
       "...                                                  ...  \n",
       "74563   perfect balance here, in this soft serene image.  \n",
       "74564  very nice indeed. the sharpness and contrast a...  \n",
       "74565                  nice tones and color for balance.  \n",
       "74566          i like the bold colors. nice sharp image.  \n",
       "74567  i am an aries and just flat out liked this ide...  \n",
       "\n",
       "[74568 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['img_path'] = train_data['img_path'].apply(lambda x: x.replace('./','../open/'))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b211ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 214748364800 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 모델, 손실함수, 옵티마이저\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBaseModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m criterion1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn [13], line 12\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39mmodules)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Image quality assessment head\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m327680\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m163840\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m163840\u001b[39m,\u001b[38;5;241m81920\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m81920\u001b[39m,\u001b[38;5;241m40910\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((out_features, in_features), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 214748364800 bytes."
     ]
    }
   ],
   "source": [
    "# 데이터셋 및 DataLoader 생성\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_dataset = CustomDataset(train_data, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True,pin_memory=True)\n",
    "\n",
    "# 모델, 손실함수, 옵티마이저\n",
    "model = BaseModel().to(device)\n",
    "criterion1 = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG['LR'])\n",
    "\n",
    "# 학습\n",
    "model.train()\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for imgs, mos in loop:\n",
    "        imgs, mos = imgs.float().to(device), mos.float().to(device)\n",
    "\n",
    "        # Forward & Loss\n",
    "        predicted_mos = model(imgs)\n",
    "        loss = criterion1(predicted_mos.squeeze(1), mos)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} finished with average loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0750b5e",
   "metadata": {},
   "source": [
    "## Inference & Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a4ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../open/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['img_path'] = test_data['img_path'].apply(lambda x: x.replace('./','../open/'))\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eaa47e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_data, transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predicted_mos_list = []\n",
    "\n",
    "# 추론 과정\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in tqdm(test_loader):\n",
    "        imgs = imgs.float().cuda()\n",
    "        mos = model(imgs)\n",
    "        predicted_mos_list.append(mos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000ed34-0a4b-42ed-8b54-264ad64e8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in tqdm(predicted_mos_list):\n",
    "    for j in i:\n",
    "        lst.append(np.float(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34718fde-c39a-4dfc-9dc3-64c331d658b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../Sub/submission_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf87af4-a677-46ba-89e0-2ca72b3127cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['mos'] = lst\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58effef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv('../Sub/submit_v.csv', index=False)\n",
    "\n",
    "print(\"Inference completed and results saved to submit.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8ded1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
